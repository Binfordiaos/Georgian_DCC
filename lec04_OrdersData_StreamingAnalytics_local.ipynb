{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yellomello/Georgian_DCC/blob/main/lec04_OrdersData_StreamingAnalytics_local.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rKI8YV6OaFjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video Explanation: https://youtu.be/CT0DVOraCjY"
      ],
      "metadata": {
        "id": "d7WrMFq5kDG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update #update linux\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null #download and install openjdk\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz #download spark binary (gunzip). -q: Turn off Wgetâ€™s output.\n",
        "!tar xf spark-3.2.1-bin-hadoop2.7.tgz #extract the spark package\n",
        "!pip install -q findspark #install the findspark package"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Directory for uploaded files",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "6dde38c8-259c-420e-81ee-f0b9041fb70f"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URS874vC6eTI",
        "outputId": "aa915b23-49ed-43b6-9f80-5f00b739914f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Hit:4 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n",
            "Hit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Hit:7 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:8 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:11 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,282 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,359 kB]\n",
            "Fetched 4,977 kB in 5s (931 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "execution_count": 51
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OzkH_-xNaG3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "RaWvAacQ6ndV"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DPHCbyGKawJ-"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# From Spark 2.0, SparkSession provides a common entry point for a Spark application\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "spark.conf.set('spark.sql.shuffle.partitions', 6)\n",
        "# spark.conf.set('spark.executor.memory', '2g')\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "eiMBjZVH60Xe",
        "outputId": "5179da04-61ba-415c-d88f-860857992f51"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fe4fc1eb820>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://4d32c2c9e92b:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "r_M-kSZ5aBsc",
        "outputId": "9badbe0c-82c2-42fe-9488-1a961874f023"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fe4fc1eb820>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://4d32c2c9e92b:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4pdwyqIaBsf",
        "outputId": "a9412bd9-f080-4952-ffe6-5b40bf10571d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;32manscombe.json\u001b[0m*                mnist_test.csv         \u001b[01;32mREADME.md\u001b[0m*\n",
            "california_housing_test.csv   mnist_train_small.csv\n",
            "california_housing_train.csv  \u001b[01;34morders_data\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "%ls sample_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmir4lbvbBFI",
        "outputId": "f658a054-5048-4d95-889e-ea40d55e7310"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z36kQBjlbqrd"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/drive/MyDrive/orders_data.zip -d sample_data/orders_data"
      ],
      "metadata": {
        "id": "m6ODI1Xo85nZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b141498-b396-4bef-c7c1-44477c3593f9"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace sample_data/orders_data/orders_1.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "6dde38c8-259c-420e-81ee-f0b9041fb70f",
          "showTitle": true,
          "title": "Directory for uploaded files"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90lBDplhaBsg",
        "outputId": "ebb438ce-7489-4e9f-c9c0-59f1585b7919"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "orders_100.csv  orders_28.csv  orders_46.csv  orders_64.csv  orders_82.csv\n",
            "orders_10.csv   orders_29.csv  orders_47.csv  orders_65.csv  orders_83.csv\n",
            "orders_11.csv   orders_2.csv   orders_48.csv  orders_66.csv  orders_84.csv\n",
            "orders_12.csv   orders_30.csv  orders_49.csv  orders_67.csv  orders_85.csv\n",
            "orders_13.csv   orders_31.csv  orders_4.csv   orders_68.csv  orders_86.csv\n",
            "orders_14.csv   orders_32.csv  orders_50.csv  orders_69.csv  orders_87.csv\n",
            "orders_15.csv   orders_33.csv  orders_51.csv  orders_6.csv   orders_88.csv\n",
            "orders_16.csv   orders_34.csv  orders_52.csv  orders_70.csv  orders_89.csv\n",
            "orders_17.csv   orders_35.csv  orders_53.csv  orders_71.csv  orders_8.csv\n",
            "orders_18.csv   orders_36.csv  orders_54.csv  orders_72.csv  orders_90.csv\n",
            "orders_19.csv   orders_37.csv  orders_55.csv  orders_73.csv  orders_91.csv\n",
            "orders_1.csv    orders_38.csv  orders_56.csv  orders_74.csv  orders_92.csv\n",
            "orders_20.csv   orders_39.csv  orders_57.csv  orders_75.csv  orders_93.csv\n",
            "orders_21.csv   orders_3.csv   orders_58.csv  orders_76.csv  orders_94.csv\n",
            "orders_22.csv   orders_40.csv  orders_59.csv  orders_77.csv  orders_95.csv\n",
            "orders_23.csv   orders_41.csv  orders_5.csv   orders_78.csv  orders_96.csv\n",
            "orders_24.csv   orders_42.csv  orders_60.csv  orders_79.csv  orders_97.csv\n",
            "orders_25.csv   orders_43.csv  orders_61.csv  orders_7.csv   orders_98.csv\n",
            "orders_26.csv   orders_44.csv  orders_62.csv  orders_80.csv  orders_99.csv\n",
            "orders_27.csv   orders_45.csv  orders_63.csv  orders_81.csv  orders_9.csv\n"
          ]
        }
      ],
      "source": [
        "%ls sample_data/orders_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bk8bJHuuaBsg",
        "outputId": "aeb1066e-9232-4abc-c448-c2cebb57076c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.types.StructType'>\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "file_location = \"sample_data/orders_data/orders_*.csv\"\n",
        "\n",
        "# create schema\n",
        "ordersSchema = StructType([\n",
        "  StructField(\"Order_ID\", DoubleType(), True),\n",
        "  StructField(\"Country\", StringType(), True),\n",
        "  StructField(\"Province\", StringType(), True),\n",
        "  StructField(\"City\", StringType(), True),\n",
        "  StructField(\"Latitude\", DoubleType(), True),\n",
        "  StructField(\"Longitude\", DoubleType(), True),\n",
        "  StructField(\"TimeStamp\", StringType(), True),\n",
        "  StructField(\"Sales_Volume\", DoubleType(), True)]\n",
        ")\n",
        "\n",
        "print(type(ordersSchema))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gfGAiDhaBsg",
        "outputId": "85ab970e-15ca-4348-9acd-182989bccfad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "# create streaming DF\n",
        "streamingDF = (\n",
        "  spark\n",
        "    .readStream\n",
        "    .schema(ordersSchema)              # schema for the streaming DF\n",
        "    .option(\"maxFilesPerTrigger\", 1)  # Form a stream by triggering one file at a time\n",
        "    .csv(file_location)\n",
        ")\n",
        "\n",
        "# Is it a streaming DF?\n",
        "print(streamingDF.isStreaming)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "892fca5b-6efd-4e04-a8ea-fbcf01688881",
          "showTitle": true,
          "title": "Import Data"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NjimzDCaBsh",
        "outputId": "19aaab7a-4a0c-4080-c7f7-156427b0e3f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "# get total sales volume group by Province\n",
        "streamingVolumeDF = (\n",
        "  streamingDF\n",
        "    .select(\"Province\", \"Sales_Volume\")\n",
        "    .groupBy(\"Province\")\n",
        "    .sum()\n",
        ")\n",
        "\n",
        "\n",
        "print(streamingVolumeDF.isStreaming)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "cd6a63d2-0c82-4bed-a3be-de4ad7bcd045",
          "showTitle": false,
          "title": ""
        },
        "id": "biA3Pc0PaBsh"
      },
      "outputs": [],
      "source": [
        "query = (\n",
        "  streamingVolumeDF\n",
        "    .writeStream\n",
        "    .format(\"memory\")        # store in-memory table\n",
        "    .queryName(\"orders_stream4\")\n",
        "    .outputMode(\"complete\")\n",
        "    .start()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "e0a33156-44a6-4bb9-b803-411070400cd1",
          "showTitle": true,
          "title": "SQL Query & Visualize"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n52Qk0h5aBsh",
        "outputId": "8e82fc0f-00cd-49fb-fd16-028a32c7a679"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------------------+\n",
            "|Province| sum(Sales_Volume)|\n",
            "+--------+------------------+\n",
            "|      MB|            798.81|\n",
            "|      SK|           2204.96|\n",
            "|      NB| 770.8499999999999|\n",
            "|      AB|           35589.0|\n",
            "|      NL|            126.84|\n",
            "|      BC|2119.1000000000004|\n",
            "|      NS| 965.5000000000001|\n",
            "|      ON| 62384.29999999998|\n",
            "|      QC|           7313.57|\n",
            "+--------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql('''\n",
        " SELECT *\n",
        " FROM orders_stream4\n",
        "\n",
        "\n",
        " ''').show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "notebookName": "Orders Data - Streaming Analytics",
      "notebookOrigID": 120418280266557,
      "widgets": {}
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}