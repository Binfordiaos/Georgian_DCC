{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yellomello/Georgian_DCC/blob/main/lec04_OrdersData_BatchAnalytics_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video Explanation: https://youtu.be/QbuToSGIei0"
      ],
      "metadata": {
        "id": "ixbLSQ9HKoaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update #update linux\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null #download and install openjdk\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz #download spark binary (gunzip). -q: Turn off Wgetâ€™s output.\n",
        "!tar xf spark-3.2.1-bin-hadoop2.7.tgz #extract the spark package\n",
        "!pip install -q findspark #install the findspark package"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Directory for uploaded files",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "6dde38c8-259c-420e-81ee-f0b9041fb70f"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URS874vC6eTI",
        "outputId": "af8ceaeb-db77-4fe1-d828-2adaf1a40912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Hit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu focal-backports InRelease\n",
            "Hit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:9 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Hit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:11 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "laDTaL8ALJ2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "RaWvAacQ6ndV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# From Spark 2.0, SparkSession provides a common entry point for a Spark application\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "spark.conf.set('spark.sql.shuffle.partitions', 6)\n",
        "# spark.conf.set('spark.executor.memory', '2g')\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "eiMBjZVH60Xe",
        "outputId": "b3ba25bb-208d-4721-d788-02298afea329"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fae4c179810>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://db44dacfa8e2:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dv4xa7eP8TUm",
        "outputId": "a9b1f08a-336c-44a5-fd8f-caa6d0f808d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive\t     spark-3.2.1-bin-hadoop2.7\t    spark-3.2.1-bin-hadoop2.7.tgz.1\n",
            "sample_data  spark-3.2.1-bin-hadoop2.7.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls sample_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSCAN0kW8cQw",
        "outputId": "a1bc2567-f8c6-4711-fef0-cdfd5157234c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "anscombe.json\t\t      mnist_test.csv\t     README.md\n",
            "california_housing_test.csv   mnist_train_small.csv\n",
            "california_housing_train.csv  orders_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fObJbdOjBEIk",
        "outputId": "259c93bf-6fec-4aa9-b953-cad93f796faa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/drive/MyDrive/orders_data.zip -d sample_data/orders_data"
      ],
      "metadata": {
        "id": "m6ODI1Xo85nZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls sample_data/orders_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEdHIsor9c4F",
        "outputId": "827e93f6-dc19-4949-ca71-54d7508e6993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "orders_100.csv\torders_28.csv  orders_46.csv  orders_64.csv  orders_82.csv\n",
            "orders_10.csv\torders_29.csv  orders_47.csv  orders_65.csv  orders_83.csv\n",
            "orders_11.csv\torders_2.csv   orders_48.csv  orders_66.csv  orders_84.csv\n",
            "orders_12.csv\torders_30.csv  orders_49.csv  orders_67.csv  orders_85.csv\n",
            "orders_13.csv\torders_31.csv  orders_4.csv   orders_68.csv  orders_86.csv\n",
            "orders_14.csv\torders_32.csv  orders_50.csv  orders_69.csv  orders_87.csv\n",
            "orders_15.csv\torders_33.csv  orders_51.csv  orders_6.csv   orders_88.csv\n",
            "orders_16.csv\torders_34.csv  orders_52.csv  orders_70.csv  orders_89.csv\n",
            "orders_17.csv\torders_35.csv  orders_53.csv  orders_71.csv  orders_8.csv\n",
            "orders_18.csv\torders_36.csv  orders_54.csv  orders_72.csv  orders_90.csv\n",
            "orders_19.csv\torders_37.csv  orders_55.csv  orders_73.csv  orders_91.csv\n",
            "orders_1.csv\torders_38.csv  orders_56.csv  orders_74.csv  orders_92.csv\n",
            "orders_20.csv\torders_39.csv  orders_57.csv  orders_75.csv  orders_93.csv\n",
            "orders_21.csv\torders_3.csv   orders_58.csv  orders_76.csv  orders_94.csv\n",
            "orders_22.csv\torders_40.csv  orders_59.csv  orders_77.csv  orders_95.csv\n",
            "orders_23.csv\torders_41.csv  orders_5.csv   orders_78.csv  orders_96.csv\n",
            "orders_24.csv\torders_42.csv  orders_60.csv  orders_79.csv  orders_97.csv\n",
            "orders_25.csv\torders_43.csv  orders_61.csv  orders_7.csv   orders_98.csv\n",
            "orders_26.csv\torders_44.csv  orders_62.csv  orders_80.csv  orders_99.csv\n",
            "orders_27.csv\torders_45.csv  orders_63.csv  orders_81.csv  orders_9.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "file_location = \"sample_data/orders_data/orders_*.csv\"\n",
        "\n",
        "# create schema\n",
        "ordersSchema = StructType([\n",
        "  StructField(\"Order_ID\", DoubleType(), True),\n",
        "  StructField(\"Country\", StringType(), True),\n",
        "  StructField(\"Province\", StringType(), True),\n",
        "  StructField(\"City\", StringType(), True),\n",
        "  StructField(\"Latitude\", DoubleType(), True),\n",
        "  StructField(\"Longitude\", DoubleType(), True),\n",
        "  StructField(\"TimeStamp\", StringType(), True),\n",
        "  StructField(\"Sales_Volume\", DoubleType(), True)])\n",
        "\n",
        "# create DataFrame\n",
        "data = (\n",
        "  spark\n",
        "    .read\n",
        "    .schema(ordersSchema)\n",
        "    .csv(file_location)\n",
        ")\n",
        "\n",
        "\n",
        "# create table for SQL analytics\n",
        "data.createOrReplaceTempView(\"orders\")\n",
        "\n",
        "data.show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Import Data",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "892fca5b-6efd-4e04-a8ea-fbcf01688881"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mIAP-t96eTL",
        "outputId": "079c914c-96ed-46a6-bba8-435d24a2253a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------+--------+--------------------+---------+---------+-------------------+------------+\n",
            "|Order_ID|Country|Province|                City| Latitude|Longitude|          TimeStamp|Sales_Volume|\n",
            "+--------+-------+--------+--------------------+---------+---------+-------------------+------------+\n",
            "|231542.0| Canada|      AB|             Calgary|-113.9835|-113.9389|2022/04/22 08:28:49|       41.49|\n",
            "|473450.0| Canada|      AB|            Edmonton|-113.4467|-113.3654|2022/04/22 05:04:24|       48.52|\n",
            "|376604.0| Canada|      AB|        Medicine Hat|-110.5798|-110.4884|2022/04/22 18:14:14|       60.79|\n",
            "|440105.0| Canada|      AB|       Sherwood Park|-113.2427| -113.242|2022/04/22 11:27:20|       77.81|\n",
            "|483058.0| Canada|      AB|            Beaumont|-113.3783|-113.2894|2022/04/22 21:04:24|       12.06|\n",
            "|303271.0| Canada|      AB|            Edmonton|-113.3609|-113.3519|2022/04/22 06:32:45|        99.0|\n",
            "|444942.0| Canada|      AB|       Fort Mcmurray|-111.4019|-111.4018|2022/04/22 10:44:56|       11.83|\n",
            "|477498.0| Canada|      NB|Macdougall Settle...| -64.7586| -64.6739|2022/04/22 04:11:29|        13.5|\n",
            "|250410.0| Canada|      AB|       Sherwood Park|-113.2142|-113.1836|2022/04/22 10:51:42|       48.38|\n",
            "|317257.0| Canada|      AB|         Fort Mackay|-111.4923|-111.4908|2022/04/22 03:45:16|       11.91|\n",
            "|271051.0| Canada|      ON|              Milton| -79.8018| -79.7156|2022/04/22 02:51:53|       80.74|\n",
            "|242527.0| Canada|      AB|             Calgary|-113.9316|-113.8966|2022/04/22 09:42:40|        61.4|\n",
            "|363883.0| Canada|      ON|            Brampton|  -79.712| -79.6918|2022/04/22 23:28:53|       68.76|\n",
            "|432813.0| Canada|      ON|       Richmond Hill| -79.3341|  -79.325|2022/04/22 15:50:34|       59.05|\n",
            "|449879.0| Canada|      QC|               Laval| -73.7235| -73.6431|2022/04/22 22:59:29|       92.37|\n",
            "|455167.0| Canada|      NS|              Sydney|  -60.119| -60.1075|2022/04/22 18:59:53|       89.23|\n",
            "|219972.0| Canada|      ON|       Richmond Hill| -79.3696| -79.2793|2022/04/22 14:34:03|        5.22|\n",
            "|475590.0| Canada|      ON|             Windsor| -83.0281| -82.9607|2022/04/22 20:11:19|        1.35|\n",
            "|213890.0| Canada|      BC|            Richmond|-122.9574|-122.8935|2022/04/22 18:11:25|        8.37|\n",
            "|302898.0| Canada|      QC|        Charlesbourg| -71.2374| -71.1506|2022/04/22 01:54:45|       30.06|\n",
            "+--------+-------+--------+--------------------+---------+---------+-------------------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_sql = spark.sql(\"SELECT count(*) FROM orders\")\n",
        "df_sql.show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "438d9de3-16e5-4b38-a538-6a7e21b20c79"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4OI-TYT6eTN",
        "outputId": "641f7f2e-9bc1-4716-c742-2f6d578e0c68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|   20000|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_sql = spark.sql(\"\"\"\n",
        "select\n",
        "  Province,\n",
        "  round(sum(Sales_Volume)) as Sales_Volume\n",
        "from orders\n",
        "group by Province\n",
        "order by Sales_Volume DESC\n",
        "\"\"\")\n",
        "df_sql.show()\n"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "SQL Query & Visualize",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "e0a33156-44a6-4bb9-b803-411070400cd1"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXbMTQEk6eTO",
        "outputId": "e078c289-9ed8-4e1d-e5dd-eb76397770e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------------+\n",
            "|Province|Sales_Volume|\n",
            "+--------+------------+\n",
            "|      ON|    431850.0|\n",
            "|      AB|    397173.0|\n",
            "|      QC|     73407.0|\n",
            "|      SK|     37867.0|\n",
            "|      BC|     37501.0|\n",
            "|      MB|     13708.0|\n",
            "|      NB|     11314.0|\n",
            "|      NS|      8899.0|\n",
            "|      NL|      2864.0|\n",
            "|      NT|       137.0|\n",
            "|      YT|       132.0|\n",
            "|      PE|        41.0|\n",
            "+--------+------------+\n",
            "\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "fcc55e84-2ce1-4272-8ebe-be5c9dfc01a9"
        },
        "id": "4ylGtR8q6eTP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "D8BMj6KwDpGv"
      }
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "Orders Data - Batch Analytics",
      "dashboards": [],
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "language": "python",
      "widgets": {},
      "notebookOrigID": 1469406854226421
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}